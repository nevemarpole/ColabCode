{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generative_Agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1CqQ6wR-ST5-Mhijxn5eLJpC2Ov2ZnI2p",
      "authorship_tag": "ABX9TyP7jqYhkg0LHQpEbmH1SplR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nevemarpole/DissertationProject/blob/main/Generative_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9JAtNPI9QVh"
      },
      "source": [
        "#Install necessary libraries\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDskfhk29VHb"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import numpy as np\n",
        "import random\n",
        "from math import log10\n",
        "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3X_wgl29W5w"
      },
      "source": [
        "#Fetch name of GPU in use\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "#Set device to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0wsgTHq9fN1"
      },
      "source": [
        "#Downloads\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "#Load in saved GPT-2 model\n",
        "gpt2_model = pickle.load(open('drive/MyDrive/Colab Notebooks/Dissertation/GPT-2/finetuned_GPT2_model.pkl', 'rb'))\n",
        "gpt2_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XFWzUO69b2O"
      },
      "source": [
        "#Tokenizes, converts all text to lower case, \n",
        "#removes stop words, stems the text passed to it\n",
        "def rework_text(text, token, stem):\n",
        "\n",
        "  #Tokenize the text to make it a list of lists\n",
        "  #Allowing words to be accessed indivdually\n",
        "  if token == True:\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    changed_text = []\n",
        "    for string in text:\n",
        "        changed_text.append(tokenizer.tokenize(string))\n",
        "        \n",
        "    text = changed_text\n",
        "\n",
        "    print(\"Text tokenized\")\n",
        "\n",
        "\n",
        "\n",
        "  if stem == True:\n",
        "    #Remove stop words and make all charcters lower case\n",
        "    documents = []\n",
        "    english_stopwords = stopwords.words('english')\n",
        "    for tok_document in text:\n",
        "        documents.append([word.lower() for word in tok_document if word not in english_stopwords])\n",
        "    \n",
        "    #Stem all words\n",
        "    stemmed_documents = []\n",
        "    for part in documents:\n",
        "        stemmed_documents.append([sb_stemmer.stem(word) for word in part]) \n",
        "    changed_text = stemmed_documents\n",
        "\n",
        "    print(\"Stop words removed and stemmed\")\n",
        "\n",
        "  return changed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnqb1Sjo90IO"
      },
      "source": [
        "#Say hello to the user\n",
        "print('Hi there!')\n",
        "\n",
        "\n",
        "#Until the user asks to stop the chatbot will talk to them\n",
        "stop = False\n",
        "\n",
        "while not stop:\n",
        "\n",
        "  found = False\n",
        "  text = []\n",
        "  \n",
        "  #Ask the user for an input\n",
        "  user_input = input('Please enter a sentence, or to bring this conversation to an end ask me to stop:\\n')\n",
        "  \n",
        "\n",
        "  #Lower and tokenize the user's input to make it workable\n",
        "  user_input = user_input.lower()\n",
        "  tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "  input_tokenized = []\n",
        "  input_tokenized.append(tokenizer.tokenize(user_input))\n",
        "  \n",
        "  #As input_tokenized makes a list of lists use this to just get\n",
        "  #the list of words as tokens\n",
        "  token_list = input_tokenized[0]\n",
        "\n",
        "  #If the input is stop then end the conversation\n",
        "  if token_list[0] == \"stop\" and len(token_list) == 1:\n",
        "      print(\"Goodbye!\")\n",
        "      stop = True\n",
        "\n",
        "\n",
        "  #The input isn't stop so a response must be chosen        \n",
        "  if stop == False: \n",
        "      #Set to the correct tokenizer\n",
        "      tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "      \n",
        "      #Make sure model is in evaluation mode\n",
        "      gpt2_model.eval()\n",
        "\n",
        "      output = []\n",
        "\n",
        "      #enocde the user input for the model to understand\n",
        "      encoded = torch.tensor(tokenizer.encode(user_input)).unsqueeze(0)\n",
        "      encoded = generated.to(device)\n",
        "\n",
        "      #Sets parameters for model generation\n",
        "      #Here the generation is done based on the user's encoded input\n",
        "      #it will look forward 100 places when search for words to use to construct\n",
        "      #it will be capped at a length of 40, and only return 1 generated sequence\n",
        "      sample_outputs = gpt2_model.generate(\n",
        "                                      encoded, \n",
        "                                      do_sample=True,   \n",
        "                                      top_k=100, \n",
        "                                      max_length = 40,\n",
        "                                      top_p=0.95, \n",
        "                                      num_return_sequences=1\n",
        "                                      )\n",
        "      \n",
        "      #Decode the output from numbers to string\n",
        "      #And remove the user input which is at the beginning\n",
        "      output = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "      output = output.replace(user_input,'')\n",
        "      print(output, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
