{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert_Finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "16zAGlKR44laDAIdd4ie4VB_7cldaGPKv",
      "authorship_tag": "ABX9TyMDe5wa5ugspoongrRuUzKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nevemarpole/DissertationProject/blob/main/Bert_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5TdNQnENpwG"
      },
      "source": [
        "#Install necessary libraries\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_SOEkO5NNld"
      },
      "source": [
        "#Imports\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgYS7V03SsTB"
      },
      "source": [
        "#Fetch name of GPU in use\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "#Set device to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNjefw_VNWse"
      },
      "source": [
        "#Read in data files\n",
        "df_train = pd.read_csv(\"drive/MyDrive/Colab Notebooks/Dissertation/Data/train.csv\", delimiter=',', usecols=('conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance')) \n",
        "df_valid = pd.read_csv(\"drive/MyDrive/Colab Notebooks/Dissertation/Data/valid.csv\", delimiter=',', usecols=('conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance')) \n",
        "df_test = pd.read_csv(\"drive/MyDrive/Colab Notebooks/Dissertation/Data/test.csv\", delimiter=',', usecols=('conv_id', 'utterance_idx', 'context', 'prompt', 'speaker_idx', 'utterance'))\n",
        "\n",
        "print(\"Data read in\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-nX5_LtLpyh"
      },
      "source": [
        "#Add the tags the BERT model will expect\n",
        "def prepareData(dataFrame):\n",
        "    sentences = dataFrame.prompt.values\n",
        "    \n",
        "    i = 0\n",
        "    for this in sentences:\n",
        "        sentences[i] = str(sentences[i])\n",
        "        i = i + 1\n",
        "\n",
        "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Convert string labels into numbers\n",
        "def prepareLabels(dataFrame):\n",
        "    dataFrame['context'].replace({\"surprised\": \"0\", \"excited\": \"1\", \"angry\": \"2\", \"proud\": \"3\", \n",
        "                             \"sad\": \"4\", \"annoyed\": \"5\", \"grateful\": \"6\", \"lonely\": \"7\", \n",
        "                             \"afraid\": \"8\", \"terrified\": \"9\", \"guilty\": \"10\", \"impressed\": \"11\",\n",
        "                             \"disgusted\": \"12\", \"hopeful\": \"13\", \"confident\": \"14\", \n",
        "                             \"furious\": \"15\", \"anxious\": \"16\", \"anticipating\": \"17\",\n",
        "                             \"joyful\": \"18\", \"nostalgic\": \"19\", \"disappointed\": \"20\",\n",
        "                             \"prepared\": \"21\", \"jealous\": \"22\", \"content\": \"23\",\n",
        "                             \"devastated\": \"24\", \"embarrassed\": \"25\", \"caring\": \"26\",\n",
        "                             \"sentimental\": \"27\", \"trusting\": \"28\", \"ashamed\": \"29\",\n",
        "                             \"apprehensive\": \"30\", \"faithful\": \"31\",}, inplace=True)\n",
        "    \n",
        "    labels = dataFrame.context.values\n",
        "    labels = np.array(labels, dtype='float32')\n",
        "    \n",
        "    return labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Tokenize, convert and pad the data\n",
        "def tokenizeData(data):\n",
        "    #Use HuggingFace's BERT tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    \n",
        "    #The length sentences will be padded or cut to\n",
        "    MAX_LEN = 128\n",
        "    \n",
        "    #Tokenize\n",
        "    tokenized = [tokenizer.tokenize(section) for section in data]\n",
        "    \n",
        "    #Words converted to IDs and padding added, or input shortened\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    return input_ids \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "#BERT uses attention masks to know which inputs to look at and which to ignore\n",
        "def applyMasks(input_ids):\n",
        "    attention_masks = []\n",
        "    \n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    \n",
        "    return attention_masks \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Function to calculate time taken for finetuning\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    #Round to second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    #Format\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))    \n",
        "    \n",
        "  \n",
        "    \n",
        "\n",
        "#Calculate the accuracy of predictions vs. labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Add opening and closing tokens to each bit of data\n",
        "train_data = prepareData(df_train)\n",
        "valid_data = prepareData(df_valid)\n",
        "test_data = prepareData(df_test)\n",
        "\n",
        "print(\"Data prepared\")\n",
        "\n",
        "\n",
        "#Change labels from strings to numbers\n",
        "train_labels = prepareLabels(df_train)\n",
        "valid_labels = prepareLabels(df_valid)\n",
        "test_labels = prepareLabels(df_test)\n",
        "\n",
        "print(\"Labels prepared\")\n",
        "\n",
        "\n",
        "#Tokenize, convert and pad data\n",
        "print(\"Loading BERT tokenizer:\")\n",
        "input_ids_train = tokenizeData(train_data)\n",
        "input_ids_valid = tokenizeData(valid_data)\n",
        "input_ids_test = tokenizeData(test_data)\n",
        "\n",
        "print(\"Input ID's configured\")\n",
        "\n",
        "\n",
        "#Apply masks to data\n",
        "attention_masks_train = applyMasks(input_ids_train)\n",
        "attention_masks_valid = applyMasks(input_ids_valid)\n",
        "attention_masks_test = applyMasks(input_ids_test)\n",
        "\n",
        "print(\"Masks applied\")\n",
        "\n",
        "\n",
        "#convert data into tensors\n",
        "train_inputs = torch.tensor(input_ids_train)\n",
        "validation_inputs = torch.tensor(input_ids_valid)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(valid_labels)\n",
        "train_masks = torch.tensor(attention_masks_train)\n",
        "validation_masks = torch.tensor(attention_masks_valid)\n",
        "\n",
        "print(\"Tensors created\")\n",
        "\n",
        "\n",
        "#Batch size for finetuning\n",
        "batch_size = 32\n",
        "\n",
        "#Create Training and Validation dataloader\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "#Load BERT model\n",
        "print(\"Loading Model:\")\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=32)\n",
        "model.to(device)\n",
        "\n",
        "#Set parameters to pass to HuggingFace's Adam\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.02},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.02}\n",
        "]\n",
        "\n",
        "#Use BERTAdam, set learning rate\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "                     lr=2e-5,\n",
        "                     warmup=.1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54mPwV1p_V3H"
      },
      "source": [
        "###                ###\n",
        "###   FINETUNING   ###\n",
        "###                ###\n",
        "\n",
        "t = []\n",
        "accuracy_info = []\n",
        "\n",
        "#Keeps track of overall loss\n",
        "train_loss_set = []\n",
        "\n",
        "#Number of training epochs\n",
        "epochs = 1\n",
        "\n",
        "#For the number of epochs\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  print(\"\\n\")\n",
        "\n",
        "  ## TRAINING ##\n",
        "  \n",
        "  #Set model into training mode\n",
        "  model.train()\n",
        "  \n",
        "  #Variable to track progress\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples = 0\n",
        "  nb_tr_steps = 0\n",
        "  start_time = time.time()\n",
        "  \n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    \n",
        "    #If this batch is multiple of 50 calculate and print out elapsed time\n",
        "    if step % 50 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - start_time)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "    \n",
        "    #Add batch to GPU\n",
        "    batch = tuple(t.to(device, dtype=torch.int64) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    #Stop gradients from accumulating\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #Forward pass\n",
        "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    train_loss_set.append(loss.item())    \n",
        "    \n",
        "    #Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    #Step using parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    #Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  #Print results from this epoch\n",
        "  print(\"\")\n",
        "  print(\"Train loss: {}\".format(tr_loss/len(train_dataloader)))\n",
        "  time_taken = format_time(time.time() - start_time)\n",
        "  print(\"Training epcoh took:\", time_taken)\n",
        "    \n",
        "\n",
        "  ## VALIDATION ##\n",
        "\n",
        "  #Put model in evaluation modet\n",
        "  model.eval()\n",
        "\n",
        "  #Variables to track progress\n",
        "  eval_loss = 0 \n",
        "  eval_accuracy = 0\n",
        "  nb_eval_steps = 0\n",
        "  nb_eval_examples = 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device, dtype=torch.int64) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    #Stop gradients from accumulating, feezes layers\n",
        "    with torch.no_grad():\n",
        "      #Forward pass and calculate logit predictions\n",
        "      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    #Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    #Calculate accuracy\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "\n",
        "  print(\"\\nValidation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "  # Record all statistics from this epoch.\n",
        "  accuracy_info.append(\n",
        "      {\n",
        "          'epoch': epoch + 1,\n",
        "          'Training Loss' : tr_loss,\n",
        "          'Valid. Accur.' : (eval_accuracy/nb_eval_steps),\n",
        "          'Training Time': time_taken,\n",
        "      }\n",
        "  )\n",
        "\n",
        "#Finetuning\n",
        "print(\"Completed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOH5gNRU2GPW"
      },
      "source": [
        "#Plot a figure to show training loss\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Batch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(train_loss_set)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw6zyzwhiZb8"
      },
      "source": [
        "#Floats to three decimal places\n",
        "pd.set_option('precision', 3)\n",
        "\n",
        "#Create a DataFrame from our training statistics\n",
        "df_stats = pd.DataFrame(data=accuracy_info)\n",
        "\n",
        "#Set row name to epoch\n",
        "df_stats = df_stats.set_index('epoch')\n",
        "\n",
        "#Print the tabel\n",
        "df_stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ioeL-03baTN"
      },
      "source": [
        "#Save tuned model to disk\n",
        "with open('finetuned_BERT_model.pkl', 'wb') as fid:\n",
        "     pickle.dump(model, fid)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
