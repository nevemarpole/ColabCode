{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Test.ipynb",
      "provenance": [],
      "mount_file_id": "1IJD7mljKSiC4v4y7ddwawwuhQCuw_gWP",
      "authorship_tag": "ABX9TyOd8fw7wAlaPagpNx78V9o8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nevemarpole/DissertationProject/blob/main/BERT_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9oehCrZcBZf"
      },
      "source": [
        "#Install necessary libraries\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtLQ78GZb_PK"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import csv\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWLjxRmlb83x"
      },
      "source": [
        "#Fetch name of GPU in use\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "#Set device to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tErlmTafb5Qo"
      },
      "source": [
        "print(\"Loading BERT tokenizer:\")\n",
        "b_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ZQsDMyb4lu"
      },
      "source": [
        "#Load in saved BERT model with 1 epoch finetuning\n",
        "bert_model = pickle.load(open('drive/MyDrive/Colab Notebooks/Dissertation/BERT/finetuned_BERT_model.pkl', 'rb'))\n",
        "bert_model.to(device)\n",
        "\n",
        "## OR ##\n",
        "#Load in saved BERT model with 2 epoch finetuning\n",
        "#bert_model = pickle.load(open('drive/MyDrive/Colab Notebooks/Dissertation/BERT/finetuned_BERT_model_2e.pkl', 'rb'))\n",
        "#bert_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpniIJ_aQfYI"
      },
      "source": [
        "#Add the tags the BERT model will expect\n",
        "def prepareData(dataFrame):\n",
        "    sentences = dataFrame.prompt.values\n",
        "    \n",
        "    i = 0\n",
        "    for this in sentences:\n",
        "        sentences[i] = str(sentences[i])\n",
        "        i = i + 1\n",
        "\n",
        "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Convert string labels into numbers\n",
        "def prepareLabels(dataFrame):\n",
        "    dataFrame['context'].replace({\"surprised\": \"0\", \"excited\": \"1\", \"angry\": \"2\", \"proud\": \"3\", \n",
        "                             \"sad\": \"4\", \"annoyed\": \"5\", \"grateful\": \"6\", \"lonely\": \"7\", \n",
        "                             \"afraid\": \"8\", \"terrified\": \"9\", \"guilty\": \"10\", \"impressed\": \"11\",\n",
        "                             \"disgusted\": \"12\", \"hopeful\": \"13\", \"confident\": \"14\", \n",
        "                             \"furious\": \"15\", \"anxious\": \"16\", \"anticipating\": \"17\",\n",
        "                             \"joyful\": \"18\", \"nostalgic\": \"19\", \"disappointed\": \"20\",\n",
        "                             \"prepared\": \"21\", \"jealous\": \"22\", \"content\": \"23\",\n",
        "                             \"devastated\": \"24\", \"embarrassed\": \"25\", \"caring\": \"26\",\n",
        "                             \"sentimental\": \"27\", \"trusting\": \"28\", \"ashamed\": \"29\",\n",
        "                             \"apprehensive\": \"30\", \"faithful\": \"31\",}, inplace=True)\n",
        "    \n",
        "    labels = dataFrame.context.values\n",
        "    labels = np.array(labels, dtype='float32')\n",
        "    \n",
        "    return labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Tokenize, convert and pad the data\n",
        "def tokenizeData(data):\n",
        "    #Use HuggingFace's BERT tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "    \n",
        "    #The length sentences will be padded or cut to\n",
        "    MAX_LEN = 128\n",
        "    \n",
        "    #Tokenize\n",
        "    tokenized = [tokenizer.tokenize(section) for section in data]\n",
        "    \n",
        "    #Words converted to IDs and padding added, or input shortened\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized]\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#BERT requires words to be masked to learn\n",
        "def applyMasks(input_ids):\n",
        "    attention_masks = []\n",
        "    \n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    \n",
        "    return attention_masks     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORk1Sb-kXded"
      },
      "source": [
        "def emotion_prediction(input_id, input_mask):\n",
        "\n",
        "  #create tensors   \n",
        "  prediction_inputs = torch.tensor(input_id)\n",
        "  prediction_masks = torch.tensor(input_mask)\n",
        "    \n",
        "  batch_size = 1\n",
        "\n",
        "  #Create Dataset/loader\n",
        "  prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  bert_model.eval()\n",
        "\n",
        "  #To store result\n",
        "  prediction = []\n",
        "\n",
        "  \n",
        "  for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    #Unpack dataloader tuple\n",
        "    b_input_ids, b_input_mask = batch\n",
        "    #Don't compute or store gradients\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    # Move logits to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    \n",
        "    # Store prediction\n",
        "    prediction.append(logits)    \n",
        "\n",
        "  return prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Add the tags the BERT model will expect\n",
        "def prepare_data(data):\n",
        "\n",
        "  data = str(data)\n",
        "  data = [\"[CLS] \" + data + \" [SEP]\"]\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Tokenize, convert and pad the sentence\n",
        "def tokenize_data(data):\n",
        "  #The length sentences will be padded or cut to\n",
        "  max_len = 128\n",
        "  \n",
        "  #Tokenize\n",
        "  tokenized = [b_tokenizer.tokenize(section) for section in data]\n",
        "  \n",
        "  #Words converted to IDs and padding added, or input shortened\n",
        "  input_ids = [b_tokenizer.convert_tokens_to_ids(x) for x in tokenized]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  return input_ids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Bert requires masks\n",
        "def apply_masks(input_ids):\n",
        "  attention_masks = []\n",
        "  \n",
        "  for section in input_ids:\n",
        "    sec_mask = [float(i>0) for i in section]\n",
        "    attention_masks.append(sec_mask)\n",
        "  \n",
        "  return attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_4S-AK2XYTv"
      },
      "source": [
        "emotions = []\n",
        "utterances = []\n",
        "\n",
        "#Read in test data from Rashkin. et al. file\n",
        "#with open('drive/MyDrive/Colab Notebooks/Dissertation/Data/test.csv', encoding='UTF-8') as csvDataFile:  csvReader = csv.reader(csvDataFile)\n",
        "#  for row in csvReader:\n",
        "#    emotions.append(row[2])\n",
        "#    utterances.append(row[3])\n",
        "#emotions.remove('context')\n",
        "#utterances.remove('prompt')\n",
        "\n",
        "\n",
        "## OR ##\n",
        "#Read in new test phrases file\n",
        "with open('drive/MyDrive/Colab Notebooks/Dissertation/Data/new_test_phrases.csv', encoding='UTF-8') as csvDataFile:\n",
        "  csvReader = csv.reader(csvDataFile)\n",
        "  for row in csvReader:\n",
        "    emotions.append(row[1].lower())\n",
        "    utterances.append(row[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCtS_56Yepm_"
      },
      "source": [
        "#To convert the numbered class BERT will provide to the corresponding emotion\n",
        "classes = [\"surprised\", \"excited\", \"angry\", \"proud\", \"sad\", \"annoyed\", \n",
        "           \"grateful\", \"lonely\", \"afraid\", \"terrified\", \"guilty\", \"impressed\",\n",
        "           \"disgusted\", \"hopeful\", \"confident\", \"furious\", \"anxious\", \n",
        "           \"anticipating\",\"joyful\", \"nostalgic\", \"disappointed\",\"prepared\", \n",
        "           \"jealous\", \"content\", \"devastated\", \"embarrassed\", \"caring\",\n",
        "           \"sentimental\", \"trusting\", \"ashamed\", \"apprehensive\", \"faithful\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJLAxVqGXuUt"
      },
      "source": [
        "correct = 0\n",
        "run = 0\n",
        "predicted_emotions = []\n",
        "\n",
        "for i in range(len(utterances)):  \n",
        "\n",
        "      highest_emotion = 0    \n",
        "      #Prepare input for BERT model\n",
        "      tagged = prepare_data(utterances[i])\n",
        "      input_id = tokenize_data(tagged)\n",
        "      mask = apply_masks(input_id)\n",
        "\n",
        "      predictions = emotion_prediction(input_id, mask)\n",
        "      predictions = predictions[0]\n",
        "      predictions = predictions.astype(int)\n",
        "      predictions = predictions[0]\n",
        "\n",
        "\n",
        "      #Look through the returned values from BERT\n",
        "      #The highest value is the most likely emotional match\n",
        "      for j in range(len(predictions)):\n",
        "        if predictions[j] > highest_emotion:\n",
        "          emotion_position = j\n",
        "          highest_emotion = predictions[j]    \n",
        "\n",
        "      #Fetch the label associated with this class number\n",
        "      predicted = classes[emotion_position]\n",
        "\n",
        "      #Collects total number of correct predictions\n",
        "      if predicted == emotions[i]:\n",
        "        correct = correct + 1\n",
        "\n",
        "      #Collects total number of test\n",
        "      run = run + 1\n",
        "\n",
        "      predicted_emotions.append(emotion_position)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVJuwN8V-diZ"
      },
      "source": [
        "accuracy = ((correct / run) * 100)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
