{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Retrieval_Agent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1N3vmrHm5lFvtGOMZ4tr0fYYS8dUPUlKv",
      "authorship_tag": "ABX9TyOxPw27iKHrEVYTZB36GRZA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nevemarpole/DissertationProject/blob/main/Retrieval_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9JAtNPI9QVh"
      },
      "source": [
        "#Install necessary libraries\n",
        "!pip install pytorch-pretrained-bert pytorch-nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDskfhk29VHb"
      },
      "source": [
        "#Imports\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import numpy as np\n",
        "import random\n",
        "from math import log10\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3X_wgl29W5w"
      },
      "source": [
        "#Fetch name of GPU in use\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "#Set device to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0wsgTHq9fN1"
      },
      "source": [
        "#Downloads\n",
        "print(\"Downloading stop words:\")\n",
        "nltk.download('stopwords')\n",
        "english_stopwords = stopwords.words('english')\n",
        "sb_stemmer = SnowballStemmer('english')\n",
        "\n",
        "print(\"Loading BERT tokenizer:\")\n",
        "b_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "#Load in saved BERT model\n",
        "bert_model = pickle.load(open('drive/MyDrive/Colab Notebooks/Dissertation/BERT/finetuned_BERT_model.pkl', 'rb'))\n",
        "bert_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XFWzUO69b2O"
      },
      "source": [
        "#Tokenizes, converts all text form files to lower case, \n",
        "#removes stop words, stems the text passed to it\n",
        "def rework_text(text):\n",
        "\n",
        "  #Tokenize the text to make it a list of lists\n",
        "  #Allowing words to be accessed indivdually\n",
        "  tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "  changed_text = []\n",
        "  for string in text:\n",
        "      changed_text.append(tokenizer.tokenize(string))\n",
        "      \n",
        "  text = changed_text\n",
        "\n",
        "  print(\"Text tokenized\")\n",
        "\n",
        "\n",
        "  #Remove stop words and make all charcters lower case\n",
        "  documents = []\n",
        "  english_stopwords = stopwords.words('english')\n",
        "  for tok_document in text:\n",
        "      documents.append([word.lower() for word in tok_document if word not in english_stopwords])\n",
        "  \n",
        "  #Stem all words\n",
        "  stemmed_documents = []\n",
        "  for part in documents:\n",
        "      stemmed_documents.append([sb_stemmer.stem(word) for word in part]) \n",
        "  changed_text = stemmed_documents\n",
        "\n",
        "  print(\"Stop words removed and stemmed\")\n",
        "\n",
        "  return changed_text\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Determin how similar a user input is to an utterance in the data\n",
        "def get_similarity(user_text, a_input):\n",
        "\n",
        "    #Store the user's input and the file utterance in a list\n",
        "    #Accessed one after another by the following loops\n",
        "    strings = [user_text, a_input] \n",
        "    \n",
        "    #Create vocabulary for the user's input and file data\n",
        "    vocabulary = []\n",
        "    for string in strings:\n",
        "        for item in string:\n",
        "            #index = vocabulary.index(item)\n",
        "            #vector[index] +=1\n",
        "            if item not in vocabulary:\n",
        "                vocabulary.append(item)\n",
        "    \n",
        "    #Create bag of words for the user's input and the inputs in the file\n",
        "    bow = []\n",
        "    for string in strings:\n",
        "        vector = np.zeros(len(vocabulary))\n",
        "        for item in string:\n",
        "            index = vocabulary.index(item)\n",
        "            vector[index] += 1\n",
        "        bow.append(vector)\n",
        "        \n",
        "    #Get TF-IDF, the multiple return values stored as a list\n",
        "    result = tfidf_weight(bow[0], bow[1])\n",
        "\n",
        "    #Get manhattan distance and use this to work out final similarity value\n",
        "    distance = manhattan_distance(result[0], result[1])\n",
        "    similarity = 1 / (1+distance)\n",
        "    \n",
        "    return similarity\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Works out the TF-IDF for both vectors passed to it\n",
        "def tfidf_weight(vector_1, vector_2):\n",
        "    \n",
        "    N = 2\n",
        "    tfidf_vector_1 = np.zeros(len(vector_1))\n",
        "    tfidf_vector_2 = np.zeros(len(vector_2))\n",
        "    \n",
        "    for i in range(len(vector_1)):\n",
        "        \n",
        "        term_booleans = [vector_1[i]!=0, vector_2[i]!=0]\n",
        "        n = sum(term_booleans)\n",
        "        \n",
        "        frequency_1 = vector_1[i]\n",
        "        tfidf_1 = log10(1+frequency_1) * log10(N/n)\n",
        "        tfidf_vector_1[i] = tfidf_1\n",
        "        \n",
        "        frequency_2 = vector_2[i]\n",
        "        tfidf_2 = log10(1+frequency_2) * log10(N/n)\n",
        "        tfidf_vector_2[i] = tfidf_2\n",
        "        \n",
        "    return tfidf_vector_1, tfidf_vector_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Works out the manhattan distance between the 2 vectors\n",
        "def manhattan_distance(vector_1, vector_2):\n",
        "    \n",
        "  distance = abs(vector_1 - vector_2)\n",
        "    \n",
        "  return distance.sum()\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C1DLKh39hGf"
      },
      "source": [
        "def emotion_prediction(input_id, input_mask):\n",
        "\n",
        "  #create tensors   \n",
        "  prediction_inputs = torch.tensor(input_id)\n",
        "  prediction_masks = torch.tensor(input_mask)\n",
        "    \n",
        "  #Only ever one input at a time to evaluate, so this stays at 1  \n",
        "  batch_size = 1\n",
        "\n",
        "  #Create Dataset/loader\n",
        "  prediction_data = TensorDataset(prediction_inputs, prediction_masks)\n",
        "  prediction_sampler = SequentialSampler(prediction_data)\n",
        "  prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "\n",
        "  # Put model in evaluation mode\n",
        "  bert_model.eval()\n",
        "\n",
        "  #To store result\n",
        "  prediction = []\n",
        "\n",
        "  \n",
        "  for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    #Unpack dataloader tuple\n",
        "    b_input_ids, b_input_mask = batch\n",
        "    #Don't compute or store gradients\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      logits = bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "\n",
        "    # Move logits to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    \n",
        "    # Store prediction\n",
        "    prediction.append(logits)    \n",
        "\n",
        "  return prediction\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Add the tags the BERT model will expect\n",
        "def prepare_data(data):\n",
        "\n",
        "  data = str(data)\n",
        "  data = [\"[CLS] \" + data + \" [SEP]\"]\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Tokenize, convert and pad the sentence\n",
        "def tokenize_data(data):\n",
        "  #The length sentences will be padded or cut to\n",
        "  max_len = 128\n",
        "  \n",
        "  #Tokenize\n",
        "  tokenized = [b_tokenizer.tokenize(section) for section in data]\n",
        "  \n",
        "  #Words converted to IDs and padding added, or input shortened\n",
        "  input_ids = [b_tokenizer.convert_tokens_to_ids(x) for x in tokenized]\n",
        "  input_ids = pad_sequences(input_ids, maxlen=max_len, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "  return input_ids\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Bert requires masks\n",
        "def apply_masks(input_ids):\n",
        "  attention_masks = []\n",
        "  \n",
        "  for section in input_ids:\n",
        "    sec_mask = [float(i>0) for i in section]\n",
        "    attention_masks.append(sec_mask)\n",
        "  \n",
        "  return attention_masks"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIl2OMzr9uQl"
      },
      "source": [
        "#To convert the numbered class BERT will provide to the corresponding emotion\n",
        "classes = [\"surprised\", \"excited\", \"angry\", \"proud\", \"sad\", \"annoyed\", \n",
        "           \"grateful\", \"lonely\", \"afraid\", \"terrified\", \"guilty\", \"impressed\",\n",
        "           \"disgusted\", \"hopeful\", \"confident\", \"furious\", \"anxious\", \n",
        "           \"anticipating\",\"joyful\", \"nostalgic\", \"disappointed\",\"prepared\", \n",
        "           \"jealous\", \"content\", \"devastated\", \"embarrassed\", \"caring\",\n",
        "           \"sentimental\", \"trusting\", \"ashamed\", \"apprehensive\", \"faithful\"]\n",
        "\n",
        "#Arrays to hold relevant file data fields\n",
        "u_id = []\n",
        "emotions = []\n",
        "utterances = []\n",
        "\n",
        "#Read in the input comparison file's relevant fields\n",
        "with open('drive/MyDrive/Colab Notebooks/Dissertation/Data/train.csv', encoding='UTF-8') as csvDataFile:\n",
        "  csvReader = csv.reader(csvDataFile)\n",
        "  for row in csvReader:\n",
        "    u_id.append(row[1])\n",
        "    emotions.append(row[2])\n",
        "    utterances.append(row[5])\n",
        "u_id.remove('utterance_idx')\n",
        "emotions.remove('context')\n",
        "utterances.remove('utterance')\n",
        "\n",
        "with open('drive/MyDrive/Colab Notebooks/Dissertation/Data/valid.csv', encoding='UTF-8') as csvDataFile:\n",
        "  csvReader = csv.reader(csvDataFile)\n",
        "  for row in csvReader:\n",
        "    u_id.append(row[1])\n",
        "    emotions.append(row[2])\n",
        "    utterances.append(row[5])\n",
        "u_id.remove('utterance_idx')\n",
        "emotions.remove('context')\n",
        "utterances.remove('utterance')\n",
        "\n",
        "with open('drive/MyDrive/Colab Notebooks/Dissertation/Data/test.csv', encoding='UTF-8') as csvDataFile:\n",
        "  csvReader = csv.reader(csvDataFile)\n",
        "  for row in csvReader:\n",
        "    u_id.append(row[1])\n",
        "    emotions.append(row[2])\n",
        "    utterances.append(row[5])\n",
        "u_id.remove('utterance_idx')\n",
        "emotions.remove('context')\n",
        "utterances.remove('utterance')\n",
        "\n",
        "#Fix broken file data\n",
        "for i in range(len(u_id)):\n",
        "  if u_id[i] == '':\n",
        "    u_id[i] = '1'\n",
        "\n",
        "#Convert ID's from strings to numbers\n",
        "for i in range(len(u_id)):\n",
        "  u_id[i] = int(u_id[i])\n",
        "\n",
        "\n",
        "#tokenize and stem the file data\n",
        "input_sentences = rework_text(utterances)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnqb1Sjo90IO"
      },
      "source": [
        "#Say hello to the user\n",
        "print('Hi there!')\n",
        "\n",
        "\n",
        "#Until the user asks to stop the chatbot will talk to them\n",
        "stop = False\n",
        "\n",
        "while not stop:\n",
        "    \n",
        "  highest_sim = 0\n",
        "  highest_e_sim = 0\n",
        "  highest_emotion = 0\n",
        "  emotion_position = 0\n",
        "  found = False\n",
        "  text = []\n",
        "  \n",
        "  #Ask the user for an input\n",
        "  user_input = input('Please enter a sentence, or to bring this conversation to an end ask me to stop:\\n')\n",
        "  \n",
        "\n",
        "  #Lower and tokenize the user's input to make it workable\n",
        "  user_input = user_input.lower()\n",
        "  tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "  input_tokenized = []\n",
        "  input_tokenized.append(tokenizer.tokenize(user_input))\n",
        "  \n",
        "  #As input_tokenized makes a list of lists use this to just get\n",
        "  #the list of words as tokens\n",
        "  token_list = input_tokenized[0]\n",
        "\n",
        "  #If the input is stop then end the conversation\n",
        "  if token_list[0] == \"stop\" and len(token_list) == 1:\n",
        "      print(\"Goodbye!\")\n",
        "      stop = True\n",
        "\n",
        "\n",
        "  #The input isn't stop so a response must be chosen        \n",
        "  if stop == False: \n",
        "      #get the most up to date user input to classify the emotion of\n",
        "      text.append(user_input)\n",
        "      new_X = text\n",
        "\n",
        "      #Prepare input for BERT model\n",
        "      tagged = prepare_data(new_X)\n",
        "      input_id = tokenize_data(tagged)\n",
        "      mask = apply_masks(input_id)\n",
        "\n",
        "      predictions = emotion_prediction(input_id, mask)\n",
        "      predictions = predictions[0]\n",
        "      predictions = predictions.astype(int)\n",
        "      predictions = predictions[0]\n",
        "\n",
        "      #Look through the returned values from BERT\n",
        "      #The highest value is the most likely emotional match\n",
        "      for i in range(len(predictions)):\n",
        "        if predictions[i] > highest_emotion:\n",
        "          emotion_position = i\n",
        "          highest_emotion = predictions[i]\n",
        "\n",
        "      #Fetch the label associated with this class number\n",
        "      new_y = classes[emotion_position]\n",
        "\n",
        "      #This print is here to guage accuracy of classifier prediction\n",
        "      print(\"Predicted emotion:\", new_y)\n",
        "\n",
        "      #Checks for similarity in the dataset of inputs it has \n",
        "      #Keeps track of similarity to utterances with a matching emotional tag\n",
        "      #seperatly from those with a different associated emotion\n",
        "      for i in range(len(input_sentences)):\n",
        "        sim_value = get_similarity(token_list, input_sentences[i])\n",
        "        if sim_value >= highest_e_sim:\n",
        "          if new_y == emotions[i]:\n",
        "            e_match = i\n",
        "            highest_e_sim = sim_value\n",
        "        if sim_value >= highest_sim:\n",
        "          match = i\n",
        "          highest_sim = sim_value\n",
        "\n",
        "\n",
        "      #If the emotions match use this response even if it has a slightly lower\n",
        "      #similarity value\n",
        "      if highest_e_sim > 0.5:\n",
        "        found = True\n",
        "        #Only use the next utterance if it is a part of the same conversation\n",
        "        if u_id[e_match] < u_id[e_match + 1]:\n",
        "          print(utterances[e_match + 1], \"\\nData emotion:\", emotions[e_match], \"\\n\")\n",
        "        else:\n",
        "          print(utterances[e_match], \"\\nData emotion:\", emotions[e_match], \"\\n\")\n",
        "      \n",
        "      #Even if the emotions don't match, if the similairity is high then\n",
        "      #fetch the response\n",
        "      elif highest_sim > 0.7:\n",
        "        found = True\n",
        "        #Only use the next utterance if it is a part of the same conversation\n",
        "        if u_id[match] < u_id[match + 1]:\n",
        "          print(utterances[match + 1], \"\\nExpected emotion:\", emotions[match], \"\\n\")\n",
        "        else:\n",
        "          print(utterances[match], \"\\nExpected emotion:\", emotions[match], \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "  #If no response can be matched to an input the chatbot informs a user\n",
        "  #that they don't know how to responde \n",
        "  if stop == False and found == False:\n",
        "    print(\"I'm sorry I don't understand, please say something else and I'll try again!\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}